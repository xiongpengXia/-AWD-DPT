{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a340c-e833-4e47-b9aa-3bcc1173b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from pytorch_wavelets import DWT1D, IDWT1D\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "import gc\n",
    "from thop import profile, clever_format\n",
    "\n",
    "# 可逆实例归一化层 - 改进版本\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        \n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(1, 1, self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(1, 1, self.num_features))\n",
    "    \n",
    "    def forward(self, x, mode):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        return x\n",
    "    \n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = 1\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1:].detach()\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "    \n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / (self.stdev + self.eps)\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "        return x\n",
    "    \n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = (x - self.affine_bias) / (self.affine_weight + self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x\n",
    "\n",
    "# 改进的自适应小波分解模块\n",
    "class AdaptiveWaveletDecomposition(nn.Module):\n",
    "    def __init__(self, input_length, pred_length, wavelet_name='db4', level=3, \n",
    "                 channel=1, device='cpu', no_decomposition=False):\n",
    "        super().__init__()\n",
    "        self.wavelet_name = wavelet_name\n",
    "        self.level = level\n",
    "        self.channel = channel\n",
    "        self.device = device\n",
    "        self.input_length = input_length\n",
    "        self.pred_length = pred_length\n",
    "        self.no_decomposition = no_decomposition\n",
    "        \n",
    "        # 可学习的阈值参数 [通道数, 分解层数]\n",
    "        if not no_decomposition:\n",
    "            self.thresholds = nn.Parameter(torch.randn(channel, level) * 0.1)\n",
    "            # 小波系数处理网络\n",
    "            self.coeff_processor = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(channel, channel, kernel_size=3, padding=1, groups=channel),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(channel, channel, kernel_size=3, padding=1, groups=channel)\n",
    "                ) for _ in range(level + 1)\n",
    "            ])\n",
    "        else:\n",
    "            self.thresholds = None\n",
    "        \n",
    "        # 计算分解后的系数长度\n",
    "        self.input_w_dim, self.pred_w_dim = self._calculate_dimensions()\n",
    "\n",
    "    def _calculate_dimensions(self):\n",
    "        if self.no_decomposition:\n",
    "            return [self.input_length], [self.pred_length]\n",
    "        \n",
    "        try:\n",
    "            # 输入序列的系数长度\n",
    "            dwt = DWT1D(wave=self.wavelet_name, J=self.level).to(self.device)\n",
    "            dummy_input = torch.ones(1, self.channel, self.input_length).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                coeffs = dwt(dummy_input)\n",
    "                approx_len = coeffs[0].shape[-1]\n",
    "                detail_lens = [c.shape[-1] for c in coeffs[1]]\n",
    "            input_dims = [approx_len] + detail_lens\n",
    "            \n",
    "            # 预测序列的系数长度\n",
    "            dummy_pred = torch.ones(1, self.channel, self.pred_length).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                coeffs_pred = dwt(dummy_pred)\n",
    "                approx_len_pred = coeffs_pred[0].shape[-1]\n",
    "                detail_lens_pred = [c.shape[-1] for c in coeffs_pred[1]]\n",
    "            pred_dims = [approx_len_pred] + detail_lens_pred\n",
    "            \n",
    "            return input_dims, pred_dims\n",
    "        except:\n",
    "            return [self.input_length], [self.pred_length]\n",
    "\n",
    "    def transform(self, x):\n",
    "        if self.no_decomposition:\n",
    "            return x, []\n",
    "        \n",
    "        try:\n",
    "            # 执行小波分解\n",
    "            dwt = DWT1D(wave=self.wavelet_name, J=self.level).to(self.device)\n",
    "            coeffs = dwt(x)\n",
    "            cA, cD_list = coeffs\n",
    "            \n",
    "            # 处理近似系数\n",
    "            cA_processed = self.coeff_processor[0](cA)\n",
    "            \n",
    "            # 应用可学习阈值到细节系数并处理\n",
    "            cD_thresholded = []\n",
    "            for i in range(self.level):\n",
    "                cD_i = cD_list[i]\n",
    "                th = torch.sigmoid(self.thresholds[:, i]).view(1, self.channel, 1)\n",
    "                cD_i = torch.sign(cD_i) * torch.clamp(torch.abs(cD_i) - th, min=0)\n",
    "                cD_i = self.coeff_processor[i+1](cD_i)\n",
    "                cD_thresholded.append(cD_i)\n",
    "            \n",
    "            return cA_processed, cD_thresholded\n",
    "        except:\n",
    "            return x, []\n",
    "\n",
    "    def inv_transform(self, yA, yD):\n",
    "        if self.no_decomposition:\n",
    "            return yA\n",
    "        \n",
    "        try:\n",
    "            idwt = IDWT1D(wave=self.wavelet_name).to(self.device)\n",
    "            return idwt((yA, yD))\n",
    "        except:\n",
    "            return yA\n",
    "\n",
    "# 改进的高频通路：动态脉冲注意力\n",
    "class SpikeAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, spike_threshold=0.3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.spike_threshold = spike_threshold\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 动态调整头数\n",
    "        if embed_dim % num_heads != 0:\n",
    "            # 找到能整除embed_dim的最大头数\n",
    "            for n in range(num_heads, 0, -1):\n",
    "                if embed_dim % n == 0:\n",
    "                    num_heads = n\n",
    "                    break\n",
    "        self.num_heads = max(1, num_heads)\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        \n",
    "        # 改进的脉冲检测层\n",
    "        self.spike_detector = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, embed_dim * 2, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(embed_dim * 2, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 注意力层\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 残差连接\n",
    "        residual = x\n",
    "        \n",
    "        # 脉冲检测\n",
    "        x_conv = self.spike_detector(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        spike_map = x_conv\n",
    "        \n",
    "        # 识别脉冲点\n",
    "        spike_mask = spike_map.mean(dim=-1) > self.spike_threshold\n",
    "        \n",
    "        # 提取稀疏点的特征\n",
    "        sparse_idx = torch.nonzero(spike_mask, as_tuple=True)\n",
    "        \n",
    "        if len(sparse_idx[0]) == 0:\n",
    "            # 如果没有检测到脉冲，使用所有点\n",
    "            sparse_features = x.reshape(-1, self.embed_dim)\n",
    "            sparse_batch_idx = torch.arange(batch_size, device=x.device).repeat_interleave(seq_len)\n",
    "            sparse_seq_idx = torch.arange(seq_len, device=x.device).repeat(batch_size)\n",
    "        else:\n",
    "            sparse_features = x[sparse_idx]\n",
    "            sparse_batch_idx, sparse_seq_idx = sparse_idx\n",
    "        \n",
    "        # 注意力计算\n",
    "        Q = self.q_proj(sparse_features)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # 多头注意力\n",
    "        Q = Q.view(-1, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
    "        \n",
    "        K = K.reshape(self.num_heads, -1, self.head_dim)\n",
    "        V = V.reshape(self.num_heads, -1, self.head_dim)\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(-1, self.embed_dim)\n",
    "        \n",
    "        # 重建完整序列\n",
    "        output = torch.zeros_like(x)\n",
    "        if len(sparse_idx[0]) == 0:\n",
    "            output = output.view(-1, self.embed_dim)\n",
    "            output = self.out_proj(attn_output).view(batch_size, seq_len, self.embed_dim)\n",
    "        else:\n",
    "            output[sparse_idx] = self.out_proj(attn_output)\n",
    "        \n",
    "        # 残差连接和层归一化\n",
    "        output = self.norm1(output + residual)\n",
    "        \n",
    "        # 前馈网络\n",
    "        ffn_output = self.ffn(output)\n",
    "        output = self.norm2(output + ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 改进的多周期稀疏注意力模块\n",
    "class CycleAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4, period_list=[24, 168], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 动态调整头数\n",
    "        if embed_dim % num_heads != 0:\n",
    "            for n in range(num_heads, 0, -1):\n",
    "                if embed_dim % n == 0:\n",
    "                    num_heads = n\n",
    "                    break\n",
    "        self.num_heads = max(1, num_heads)\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        \n",
    "        self.period_list = period_list\n",
    "        \n",
    "        # 可学习的周期权重\n",
    "        self.period_weights = nn.Parameter(torch.ones(len(period_list)))\n",
    "        \n",
    "        # 周期嵌入\n",
    "        self.period_embedding = nn.Embedding(len(period_list), embed_dim)\n",
    "        \n",
    "        # 三种注意力机制\n",
    "        # 1. Intra-Period Attention (周期内注意力)\n",
    "        self.intra_period_attention = nn.MultiheadAttention(\n",
    "            embed_dim, self.num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 2. Inter-Period Attention (周期间注意力)\n",
    "        self.inter_period_attention = nn.MultiheadAttention(\n",
    "            embed_dim, self.num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 3. Phase Attention (相位注意力)\n",
    "        self.phase_attention = nn.MultiheadAttention(\n",
    "            embed_dim, self.num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.norm_out = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 前馈网络\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # 融合权重\n",
    "        self.intra_weight = nn.Parameter(torch.tensor(1.0))\n",
    "        self.inter_weight = nn.Parameter(torch.tensor(1.0))\n",
    "        self.phase_weight = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def fold_tensor(self, x, period):\n",
    "        \"\"\"将序列折叠为3D张量 [batch, num_cycles, period, features]\"\"\"\n",
    "        batch_size, seq_len, features = x.shape\n",
    "        num_cycles = seq_len // period\n",
    "        \n",
    "        # 截断到完整周期\n",
    "        truncated_len = num_cycles * period\n",
    "        x_truncated = x[:, :truncated_len, :]\n",
    "        \n",
    "        # 重塑为 [batch, num_cycles, period, features]\n",
    "        folded = x_truncated.view(batch_size, num_cycles, period, features)\n",
    "        \n",
    "        return folded, num_cycles\n",
    "    \n",
    "    def unfold_tensor(self, folded, target_seq_len):\n",
    "        \"\"\"将3D张量展开回2D序列\"\"\"\n",
    "        batch_size, num_cycles, period, features = folded.shape\n",
    "        unfolded = folded.reshape(batch_size, num_cycles * period, features)\n",
    "        \n",
    "        # 如果长度不匹配，填充零\n",
    "        if unfolded.size(1) < target_seq_len:\n",
    "            padding = torch.zeros(\n",
    "                batch_size, target_seq_len - unfolded.size(1), features,\n",
    "                device=unfolded.device\n",
    "            )\n",
    "            unfolded = torch.cat([unfolded, padding], dim=1)\n",
    "        \n",
    "        return unfolded\n",
    "    \n",
    "    def compute_intra_period_attention(self, folded):\n",
    "        \"\"\"计算周期内注意力\"\"\"\n",
    "        batch_size, num_cycles, period, features = folded.shape\n",
    "        \n",
    "        # 重塑为 [batch * num_cycles, period, features]\n",
    "        intra_input = folded.reshape(batch_size * num_cycles, period, features)\n",
    "        \n",
    "        # 自注意力计算\n",
    "        intra_output, _ = self.intra_period_attention(\n",
    "            intra_input, intra_input, intra_input\n",
    "        )\n",
    "        \n",
    "        # 重塑回原始形状并归一化\n",
    "        intra_output = intra_output.reshape(batch_size, num_cycles, period, features)\n",
    "        intra_output = self.norm1(intra_output)\n",
    "        \n",
    "        return intra_output\n",
    "    \n",
    "    def compute_inter_period_attention(self, folded):\n",
    "        \"\"\"计算周期间注意力\"\"\"\n",
    "        batch_size, num_cycles, period, features = folded.shape\n",
    "        \n",
    "        # 重塑为 [batch * period, num_cycles, features]\n",
    "        inter_input = folded.permute(0, 2, 1, 3).reshape(\n",
    "            batch_size * period, num_cycles, features\n",
    "        )\n",
    "        \n",
    "        # 自注意力计算\n",
    "        inter_output, _ = self.inter_period_attention(\n",
    "            inter_input, inter_input, inter_input\n",
    "        )\n",
    "        \n",
    "        # 重塑回原始形状并归一化\n",
    "        inter_output = inter_output.reshape(\n",
    "            batch_size, period, num_cycles, features\n",
    "        ).permute(0, 2, 1, 3)\n",
    "        inter_output = self.norm2(inter_output)\n",
    "        \n",
    "        return inter_output\n",
    "    \n",
    "    def compute_phase_attention(self, folded):\n",
    "        \"\"\"计算相位注意力\"\"\"\n",
    "        batch_size, num_cycles, period, features = folded.shape\n",
    "        \n",
    "        # 创建相位矩阵: [batch, phase_position, num_cycles, features]\n",
    "        phase_matrix = folded.permute(0, 2, 1, 3)  # [batch, period, num_cycles, features]\n",
    "        \n",
    "        # 计算每个相位的注意力\n",
    "        phase_outputs = []\n",
    "        for p in range(period):\n",
    "            phase_slice = phase_matrix[:, p, :, :]  # [batch, num_cycles, features]\n",
    "            phase_slice_output, _ = self.phase_attention(\n",
    "                phase_slice, phase_slice, phase_slice\n",
    "            )\n",
    "            phase_outputs.append(phase_slice_output.unsqueeze(1))\n",
    "        \n",
    "        # 组合所有相位\n",
    "        phase_output = torch.cat(phase_outputs, dim=1)  # [batch, period, num_cycles, features]\n",
    "        \n",
    "        # 转换回原始顺序并归一化\n",
    "        phase_output = phase_output.permute(0, 2, 1, 3)\n",
    "        phase_output = self.norm3(phase_output)\n",
    "        \n",
    "        return phase_output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        all_period_outputs = []\n",
    "        period_weights = torch.softmax(self.period_weights, dim=0)\n",
    "        \n",
    "        for idx, period in enumerate(self.period_list):\n",
    "            if seq_len < period * 2:  # 需要至少两个完整周期\n",
    "                continue\n",
    "            \n",
    "            # 1. 添加周期嵌入\n",
    "            period_emb = self.period_embedding(torch.tensor(idx, device=x.device))\n",
    "            period_emb = period_emb.view(1, 1, -1).expand(batch_size, seq_len, -1)\n",
    "            x_with_period = x + period_emb\n",
    "            \n",
    "            # 2. 时间折叠\n",
    "            folded, num_cycles = self.fold_tensor(x_with_period, period)\n",
    "            \n",
    "            if num_cycles < 2:  # 至少需要2个周期进行有意义的学习\n",
    "                continue\n",
    "            \n",
    "            # 3. 计算三种注意力\n",
    "            # 3.1 周期内注意力\n",
    "            intra_output = self.compute_intra_period_attention(folded)\n",
    "            \n",
    "            # 3.2 周期间注意力\n",
    "            inter_output = self.compute_inter_period_attention(folded)\n",
    "            \n",
    "            # 3.3 相位注意力\n",
    "            phase_output = self.compute_phase_attention(folded)\n",
    "            \n",
    "            # 4. 加权融合三种注意力输出\n",
    "            # 使用可学习的权重\n",
    "            weights = torch.softmax(\n",
    "                torch.stack([self.intra_weight, self.inter_weight, self.phase_weight]),\n",
    "                dim=0\n",
    "            )\n",
    "            \n",
    "            cycle_output = (\n",
    "                weights[0] * intra_output +\n",
    "                weights[1] * inter_output +\n",
    "                weights[2] * phase_output\n",
    "            )\n",
    "            \n",
    "            # 5. 展开回2D序列\n",
    "            output_2d = self.unfold_tensor(cycle_output, seq_len)\n",
    "            all_period_outputs.append(output_2d)\n",
    "        \n",
    "        if all_period_outputs:\n",
    "            # 使用周期权重合并不同周期的结果\n",
    "            weighted_outputs = []\n",
    "            for i, out in enumerate(all_period_outputs):\n",
    "                weight = period_weights[i]\n",
    "                weighted_outputs.append(weight * out)\n",
    "            \n",
    "            # 求和并归一化\n",
    "            combined_output = sum(weighted_outputs)\n",
    "            combined_output = self.norm_out(combined_output)\n",
    "        else:\n",
    "            # 如果没有有效周期，返回原始输入\n",
    "            combined_output = x\n",
    "        \n",
    "        # 残差连接\n",
    "        output = combined_output + residual\n",
    "        \n",
    "        # 前馈网络\n",
    "        ffn_output = self.ffn(output)\n",
    "        output = output + self.dropout(ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 修复的位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 修复：确保索引不超出范围\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])  # 对于奇数维度，去掉最后一个div_term\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# 修复的小波重构 + 维度融合\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    def __init__(self, input_length, pred_length, num_features, wavelet_level=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.pred_length = pred_length\n",
    "        self.num_features = num_features\n",
    "        self.wavelet_level = wavelet_level\n",
    "        \n",
    "        # 维度融合层 - 使用MLP捕获跨维度依赖\n",
    "        # 将输入展平，通过MLP，再重塑\n",
    "        flattened_size = input_length * num_features\n",
    "        \n",
    "        # Dimension-Wise Fusion Layer (MLP架构)\n",
    "        self.dim_fusion_mlp = nn.Sequential(\n",
    "            # 第一个MLP\n",
    "            nn.Linear(flattened_size, flattened_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            # 第二个MLP\n",
    "            nn.Linear(flattened_size * 2, flattened_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # 预测维度调整层\n",
    "        self.output_adjust = nn.Linear(input_length, pred_length)\n",
    "    \n",
    "    def forward(self, reconstructed_features):\n",
    "        \"\"\"\n",
    "        特征融合流程：\n",
    "        1. 输入是经过小波重构的特征 [batch, L, C]\n",
    "        2. 通过维度融合层捕获跨维度依赖\n",
    "        3. 输出预测序列 [batch, T, C]\n",
    "        \"\"\"\n",
    "        batch_size = reconstructed_features.size(0)\n",
    "        \n",
    "        # 展平特征以捕获跨维度依赖\n",
    "        flattened = reconstructed_features.reshape(batch_size, -1)\n",
    "        \n",
    "        # 通过维度融合MLP\n",
    "        fused = self.dim_fusion_mlp(flattened)\n",
    "        \n",
    "        # 重塑回 [batch, input_length, num_features]\n",
    "        reshaped = fused.reshape(batch_size, self.input_length, self.num_features)\n",
    "        \n",
    "        # 调整时间维度到预测长度\n",
    "        # 对每个特征单独处理\n",
    "        output_features = []\n",
    "        for i in range(self.num_features):\n",
    "            feature_data = reshaped[:, :, i]  # [batch, input_length]\n",
    "            # 使用线性层调整时间维度\n",
    "            adjusted_feature = self.output_adjust(feature_data)  # [batch, pred_length]\n",
    "            output_features.append(adjusted_feature.unsqueeze(2))\n",
    "        \n",
    "        # 合并所有特征\n",
    "        output = torch.cat(output_features, dim=2)  # [batch, pred_length, num_features]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 时间序列预测模型 \n",
    "class TimeSeriesForecaster(nn.Module):\n",
    "    def __init__(self, input_length, pred_length, num_features, \n",
    "                 wavelet_name='db4', wavelet_level=3, \n",
    "                 spike_heads=4, cycle_heads=4, dropout=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.pred_length = pred_length\n",
    "        self.num_features = num_features\n",
    "        self.device = device\n",
    "        \n",
    "        # 可逆实例归一化\n",
    "        self.revin = RevIN(num_features)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoder = PositionalEncoding(num_features)\n",
    "        \n",
    "        # 输入投影层\n",
    "        self.input_projection = nn.Linear(num_features, num_features)\n",
    "        \n",
    "        # 自适应小波分解模块\n",
    "        self.wavelet = AdaptiveWaveletDecomposition(\n",
    "            input_length, pred_length, wavelet_name, wavelet_level, \n",
    "            num_features, device\n",
    "        )\n",
    "        \n",
    "        # 高频通路：动态脉冲注意力\n",
    "        self.spike_attention = SpikeAttention(\n",
    "            embed_dim=num_features, \n",
    "            num_heads=spike_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 低频通路：多周期稀疏注意力\n",
    "        self.cycle_attention = CycleAttention(\n",
    "            embed_dim=num_features,\n",
    "            num_heads=cycle_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 特征融合层\n",
    "        self.feature_fusion = MultiScaleFeatureFusion(\n",
    "            input_length=input_length,\n",
    "            pred_length=pred_length,\n",
    "            num_features=num_features,\n",
    "            wavelet_level=wavelet_level,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "    \n",
    "    def forward(self, x, mode='train'):\n",
    "        # 可逆实例归一化（论文公式）\n",
    "        x_norm = self.revin(x, 'norm')\n",
    "        \n",
    "        # 输入投影和位置编码\n",
    "        x_proj = self.input_projection(x_norm)\n",
    "        x_encoded = self.pos_encoder(x_proj)\n",
    "        \n",
    "        # 转换为通道在前格式 [batch, channels, seq_len] 用于小波分解\n",
    "        x_permuted = x_encoded.permute(0, 2, 1)\n",
    "        \n",
    "        # 小波分解\n",
    "        cA, cD_list = self.wavelet.transform(x_permuted)\n",
    "        \n",
    "        # 处理近似系数 (低频) - Multi-Period Transformer\n",
    "        cA_processed = cA.permute(0, 2, 1)\n",
    "        cA_processed = self.cycle_attention(cA_processed)\n",
    "        cA_processed = cA_processed.permute(0, 2, 1)\n",
    "        \n",
    "        # 处理细节系数 (高频) - Spike Transformer\n",
    "        cD_processed_list = []\n",
    "        for cD in cD_list:\n",
    "            cD_processed = cD.permute(0, 2, 1)\n",
    "            cD_processed = self.spike_attention(cD_processed)\n",
    "            cD_processed = cD_processed.permute(0, 2, 1)\n",
    "            cD_processed_list.append(cD_processed)\n",
    "        \n",
    "        # 小波重构\n",
    "        reconstructed = self.wavelet.inv_transform(cA_processed, cD_processed_list)\n",
    "        \n",
    "        # 转换回原始格式 [batch, seq_len, features]\n",
    "        reconstructed_features = reconstructed.permute(0, 2, 1)\n",
    "        \n",
    "        # 按照论文的特征融合：维度融合层（Dimension-Wise Fusion）\n",
    "        output = self.feature_fusion(reconstructed_features.contiguous())\n",
    "        \n",
    "        # 反归一化\n",
    "        output_denorm = self.revin(output, 'denorm')\n",
    "        \n",
    "        return output_denorm\n",
    "\n",
    "# 改进的数据加载和预处理函数\n",
    "def load_etth_data(file_path):    \n",
    "    \"\"\"加载ETTh数据集并处理时间戳\"\"\"\n",
    "    df = pd.read_csv(file_path)    \n",
    "        # 转换时间戳为datetime对象\n",
    "    df['date'] = pd.to_datetime(df['date'])    \n",
    "        # 创建更丰富的时间特征\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['date'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['date'].dt.hour / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['date'].dt.day / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['date'].dt.day / 31)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date'].dt.month / 12)\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['date'].dt.weekday / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['date'].dt.weekday / 7)\n",
    "        \n",
    "        # 移除原始日期列\n",
    "    df.drop('date', axis=1, inplace=True)\n",
    "        \n",
    "    return df.values\n",
    "\n",
    "def preprocess_data(data, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):    \n",
    "    \"\"\"划分数据集并进行归一化\"\"\"    \n",
    "    # 确保比例总和为1\n",
    "    total_ratio = train_ratio + val_ratio + test_ratio\n",
    "    train_ratio /= total_ratio\n",
    "    val_ratio /= total_ratio\n",
    "    \n",
    "    # 划分数据集 (按时间顺序)\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    val_size = int(len(data) * val_ratio)\n",
    "    test_size = len(data) - train_size - val_size  \n",
    "    \n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:] \n",
    "    \n",
    "    # 归一化处理 - 使用StandardScaler\n",
    "    scalers = []    \n",
    "    for i in range(data.shape[1]):\n",
    "        scaler = StandardScaler()\n",
    "        train_data[:, i] = scaler.fit_transform(train_data[:, i].reshape(-1, 1)).flatten()\n",
    "        val_data[:, i] = scaler.transform(val_data[:, i].reshape(-1, 1)).flatten()\n",
    "        test_data[:, i] = scaler.transform(test_data[:, i].reshape(-1, 1)).flatten()\n",
    "        scalers.append(scaler)        \n",
    "    \n",
    "    return train_data, val_data, test_data, scalers\n",
    "\n",
    "def create_inout_sequences(data, input_len, output_len, stride=1):    \n",
    "    \"\"\"创建时间序列输入-输出对\"\"\"\n",
    "    X, y = [], []\n",
    "    total_len = len(data)    \n",
    "    for i in range(0, total_len - input_len - output_len + 1, stride):\n",
    "        X.append(data[i:i+input_len, :])        \n",
    "        y.append(data[i+input_len:i+input_len+output_len, :])      \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 改进的训练和评估函数\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0    \n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(data_loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 定期清理内存\n",
    "        if batch_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"评估函数，计算MSE和MAE\"\"\"\n",
    "    model.eval()\n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_targets.append(batch_y.cpu().numpy())\n",
    "            \n",
    "            # 计算MSE\n",
    "            mse = criterion(outputs, batch_y)\n",
    "            total_mse += mse.item()\n",
    "            \n",
    "            # 计算MAE\n",
    "            mae = torch.mean(torch.abs(outputs - batch_y))\n",
    "            total_mae += mae.item()\n",
    "    \n",
    "    avg_mse = total_mse / len(data_loader)\n",
    "    avg_mae = total_mae / len(data_loader)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets.flatten(), all_outputs.flatten()))\n",
    "    \n",
    "    return total_mse, total_mae, avg_mse, avg_mae, rmse\n",
    "\n",
    "# 学习率调度器\n",
    "def get_lr_scheduler(optimizer, scheduler_type='cosine', num_epochs=50):\n",
    "    if scheduler_type == 'cosine':\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    elif scheduler_type == 'step':\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    elif scheduler_type == 'plateau':\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    else:\n",
    "        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0)\n",
    "\n",
    "# 主训练函数\n",
    "def main():\n",
    "    # 设置超参数\n",
    "    input_length = 96\n",
    "    pred_length = 96\n",
    "    batch_size = 32  # 减小批大小以减少内存使用\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    dropout = 0.1\n",
    "    patience = 15\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    data = load_etth_data('../data/ETTm2.csv')\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    \n",
    "    # 预处理数据\n",
    "    train_data, val_data, test_data, scalers = preprocess_data(data)\n",
    "    print(f\"Train shape: {train_data.shape}, Val shape: {val_data.shape}, Test shape: {test_data.shape}\")\n",
    "    \n",
    "    # 创建输入-输出序列\n",
    "    X_train, y_train = create_inout_sequences(train_data, input_length, pred_length, stride=1)\n",
    "    X_val, y_val = create_inout_sequences(val_data, input_length, pred_length, stride=1)\n",
    "    X_test, y_test = create_inout_sequences(test_data, input_length, pred_length, stride=1)\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # 转换为PyTorch张量（不转移到GPU）\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.FloatTensor(y_val)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, \n",
    "                             pin_memory=True, num_workers=2)\n",
    "    \n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           pin_memory=True, num_workers=2)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                            pin_memory=True, num_workers=2)\n",
    "    \n",
    "    # 初始化模型\n",
    "    num_features = data.shape[1]\n",
    "    model = TimeSeriesForecaster(\n",
    "        input_length=input_length,\n",
    "        pred_length=pred_length,\n",
    "        num_features=num_features,\n",
    "        dropout=dropout,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    \n",
    "    # ==================== 计算模型GFLOPS和参数量 ====================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"计算模型复杂度:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 创建输入张量 [batch_size, seq_len, features]\n",
    "    batch_size_flops = 1  # 使用batch_size=1进行计算\n",
    "    dummy_input = torch.randn(batch_size_flops, input_length, num_features).to(device)\n",
    "    \n",
    "    try:\n",
    "        # 使用thop计算FLOPs和参数量\n",
    "        flops, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "        \n",
    "        # 转换为GFLOPs\n",
    "        gflops = flops / 1e9\n",
    "        \n",
    "        # 格式化输出\n",
    "        flops_formatted, params_formatted = clever_format([flops, params], \"%.3f\")\n",
    "        \n",
    "        print(f\"模型总参数量: {params_formatted}\")\n",
    "        print(f\"模型计算量 (FLOPs): {flops_formatted}\")\n",
    "        print(f\"单次前向传播计算量: {gflops:.4f} GFLOPs\")\n",
    "        print(f\"输入形状: {dummy_input.shape}\")\n",
    "        print(f\"输出形状: {model(dummy_input).shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"计算FLOPs时出错: {e}\")\n",
    "        print(\"请确保已安装thop库: pip install thop\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ==================== 计算完成 ====================\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = get_lr_scheduler(optimizer, 'plateau', num_epochs)\n",
    "    \n",
    "    # 早停机制\n",
    "    early_stopping = EarlyStopping(patience=patience, delta=0.001)\n",
    "    \n",
    "    # 训练循环\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 验证\n",
    "        val_mse, val_mae, avg_val_mse, avg_val_mae, val_rmse = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(avg_val_mse)\n",
    "        \n",
    "        # 学习率调度\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_mse)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 早停检查\n",
    "        early_stopping(avg_val_mse)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], LR: {current_lr:.6f}, '\n",
    "                  f'Train Loss: {train_loss:.6f}, Val MSE: {avg_val_mse:.6f}, '\n",
    "                  f'Val MAE: {avg_val_mae:.6f}, Val RMSE: {val_rmse:.6f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if avg_val_mse < best_val_loss:\n",
    "            best_val_loss = avg_val_mse\n",
    "            torch.save(model.state_dict(), 'best_time_series_forecaster_paper.pth')\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # 加载最佳模型进行测试\n",
    "    model.load_state_dict(torch.load('best_time_series_forecaster_paper.pth'))\n",
    "    \n",
    "    # 测试\n",
    "    test_mse, test_mae, avg_test_mse, avg_test_mae, test_rmse = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Test Results - MSE: {avg_test_mse:.6f}, MAE: {avg_test_mae:.6f}, RMSE: {test_rmse:.6f}')\n",
    "    \n",
    "    # 保存最终模型\n",
    "    torch.save(model.state_dict(), 'final_time_series_forecaster_paper.pth')\n",
    "    print(\"Model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd2399-b7c7-4bbb-91b0-26c2c12beaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
